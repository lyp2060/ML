Naive Bayes Classifier Spam and Ham

条件概率

P(A/B) 表示条件A在另外一个B下发生的概率
P(A/B) *P(B) = P(A^B)

P(A^B) 表示A，B同时发生的联合概率

P（google offer | 刷lintcode 练习题50 题）= P(Google offer ^ 刷 lint code 50题）/P(刷 lint code 50题）

分类问题： 

回归问题： 没有类别， 用来预测型

垃圾邮件分类：

P("SPAM"/"A Great Problem")

B 是已知的一系列条件向量


P(B|A) = P(A^B)/P(A) = P(A/B) *P(B)/P(A)

P(A/B)*P(B) = P(A^B) = P(B|A) *P(A)

P(A^B) 是A和B同时发生的联合概率


P(B|A)=P(A/B)*P(B)/P(A)

现实的困难
  训练集是有限的
  句子的可能性是无限的
  覆盖所有句子可能性的训练集是不存在
  

把句子拆成词？
  中文专门分词 -->动态规划实现的 viterbi algorithm 
      分词工具： gibhub.com/fxsjy/jieba
  
A problem Great ---> "A" "Great" "problem"


import pandas as pd
import numpy as np
import matploblib.pyplot as plt
df = pd.read_csv("../input/sapm.csv", encoding='latin-1')
df.head()

from sklearn.model_selection import train_test_split

data_train, data_test, label_brain, label_test = train_test_split(
  df.v2,
  df.v1,
  test_size=0.1,
  random_state = 0
)

from sklearn.feature_extraction.text import CountVectorizer

a = ['I love you. good are you', 'you are so good good good']
vectorizer = CountVectorizer()
result = vectorizer.fit_transform(a）

print(result)
print(vectorizer.vocabulary_)

(0 (第几个句子）， 0（词语编号））
(0, 0) 1
(0, 1) 1
(0, 4) 2
(0, 2) 1
(1, 3) 1
(1, 0) 1 
(1, 1) 3
(1, 4) 1

from sklearn.naive_bayers import MultinomialNB

nb = MultinomialNB()
nb.fit(data_train_count, label_train)
pred = nb.predict(data_test_count)

from sklearn.metrics import confusion_matrix, accuracy_score, classification_report

print(accuracy_score(label_test,pred))
print(confusion_matrix(label_test,pred))
print(classification_report(label_test,pred))

#cross validation
from sklearn.model_selection import cross_val_score
cv = cross_val_score(nb, data_train_count, label_train, cv=10, scoring='acccuracy')
print(corss_val)
print(np.mean(cv))

邮件标题和正文是否等价考虑？
出现在不同地方，给不同权重

有点：
快，简单
缺点
假设独立性，学习不了关联性

<<机器学习实战>>
